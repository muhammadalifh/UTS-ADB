{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice_1_Structured_Dataset_Plotting_Supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGjVo-Ex-rnA"
      },
      "source": [
        "# Introduction Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Apa itu Google Colab?</h1>\n",
        "\n",
        "Colaboratory, atau disingkat \"Colab\", memungkinkan Anda untuk menulis dan mengeksekusi Python di browser Anda, dengan\n",
        "\n",
        "- Tidak perlu konfigurasi\n",
        "- Akses gratis ke GPU\n",
        "- Berbagi dengan mudah\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAxzEtJ-kpA"
      },
      "source": [
        "# Cek versi python\r\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWRFHfWEBLO9"
      },
      "source": [
        "# Dataset Terstruktur\r\n",
        "Jenis format data yang umum digunakan:\r\n",
        "   - **CSV**: Format file sederhana yang digunakan untuk menyimpan data tabel.\r\n",
        "   - **JSON**: Terutama digunakan untuk mengirimkan data antara server dan aplikasi web.\r\n",
        "   - **Excel**: File spreadsheet yang dibuat dengan Microsoft Excel.\r\n",
        "   - **XML**: Terutama digunakan untuk menyimpan atau mengangkut data dalam aplikasi HTML. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuBx5h-r_gqT"
      },
      "source": [
        "# Pandas\r\n",
        "![image.png](https://media.chinaeducationaltours.com/image/panda/panda-1080.jpg)\r\n",
        "- *Library* Software yang ditulis untuk bahasa pemrograman Python untuk analisis data\r\n",
        "- Mengakses dan memanipulasi data dalam DataFrame dan Series\r\n",
        "- Menyediakan struktur dan operasi data untuk memanipulasi tabel numerik dan kategorik\r\n",
        "- Pandas dapat membaca data dari berbagai format seperti  **CSV**, **TSV**, **MS Excel**, dll."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPuepNloCScc"
      },
      "source": [
        "### Kegunaan Pandas:\r\n",
        "\r\n",
        "- Manipulasi data:\r\n",
        "  - *Extract column*(kolom), *Extract row*(baris)\r\n",
        "  - Grouping and Aggregation \r\n",
        "  - *Filtering* (Memilih baris berdasarkan suatu kriteria tertentu)\r\n",
        "  - Joining (Penggabungan antara dua atau lebih *DataFrame*)\r\n",
        "  - Pivoting\r\n",
        "- *Data cleaning*:\r\n",
        "  - Menghapus data yang hilang (*Missing values*)\r\n",
        "  - *Imputation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGnIdu4oNvEg"
      },
      "source": [
        "## Manipulasi Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz7t9hrHPZp8"
      },
      "source": [
        "Manipulasi data adalah teknik dasar yang digunakan pada kegiatan analisis data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMpEk6BXUwEX"
      },
      "source": [
        "### Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FhWA4eo_PSl"
      },
      "source": [
        "# Import library\r\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDhPoRUUrjH"
      },
      "source": [
        "### Creating a sample dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cFfbsy7_GEz"
      },
      "source": [
        "- Pandas terbentuk dari **list of dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLcDMdtp_Fbp"
      },
      "source": [
        "# Creating a sample dataset\r\n",
        "student = [\"Farid\", \"Rizky\", \"Anna\", \"Dani\", \"Juna\", \"Bayu\", \"Gilang\"]\r\n",
        "classroom = [\"A\",\"A\",\"B\",\"A\",\"B\",\"B\",\"B\"]\r\n",
        "age = [15,15,16,17,15,18,16]\r\n",
        "score = [80,82,95,72,67,98,85]\r\n",
        "data = {\"Student\":student, \"Class\":classroom, \"Age\":age, \"Score\":score}\r\n",
        "df = pd.DataFrame(data)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9v8x8lOCBff"
      },
      "source": [
        "- Dan juga bisa dibentuk dari **list of list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmTdueEkBMUJ"
      },
      "source": [
        "# Creating a sample dataset \r\n",
        "data=[[\"Farid\", \"A\", 15, 80,],\r\n",
        "      [\"Rizky\", \"A\", 15, 82,],\r\n",
        "      [\"Anna\", \"B\", 16, 95,],\r\n",
        "      [\"Dani\", \"A\", 17, 72,],\r\n",
        "      [\"Juna\", \"B\", 15, 67,],\r\n",
        "      [\"Bayu\", \"B\", 18, 98,],\r\n",
        "      [\"Gilang\", \"B\", 16, 85,]]\r\n",
        "columns = [\"Student\",\"Class\",\"Age\",\"Score\"]\r\n",
        "df = pd.DataFrame(data=data,columns=columns)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgIBO-D0CHDf"
      },
      "source": [
        "- Kita dapat menambahkan data (harus berupa dictionary dari setiap kolom) ke DataFrame dengan menggunakan method **\"append\"** dengan mengabaikan index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27yy_E0_wJF"
      },
      "source": [
        "# Add data (one record) into dataframe\r\n",
        "data7 = {\"Student\":\"Dila\", \"Class\":\"A\", \"Age\":18, \"Score\":82}\r\n",
        "df = df.append(data7, ignore_index=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSH1d8HuD3YZ"
      },
      "source": [
        "- Memanggil DataFrame untuk kolom tertentu dengan menggunakan pemanggilan berdasarkan list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBXzD40FKUfk"
      },
      "source": [
        "# View by Column, get DataFrame\r\n",
        "cols = ['Student','Class']\r\n",
        "df[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mauyqvUYET6F"
      },
      "source": [
        "- Jika pemanggilan menggunakan string, akan memberikan keluaran berupa Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8ca0l1-Eeqy"
      },
      "source": [
        "# View by Column, get Series\r\n",
        "cols = 'Student'\r\n",
        "df[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j207uh2zE6D6"
      },
      "source": [
        "- Memanggil DataFrame untuk baris tertentu dengan menggunakan pemanggilan berdasarkan list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2tosxgKdA7"
      },
      "source": [
        "# Select Rows By Index, get DataFrame\r\n",
        "df.iloc[2:5] # Memanggil data 2 hingga sebelum 5, atau bisa menggunakan 2:3 untuk satu data (hanya data index 2) yang tampil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gVEA3FKFzWn"
      },
      "source": [
        "- Jika pemanggilan menggunakan urutan index, akan memberikan keluaran berupa Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDgvIxn-FRzv"
      },
      "source": [
        "# Select Rows By Index, get Series\r\n",
        "df.iloc[2] # Memanggil hanya data index ke-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEonYxRfF6lr"
      },
      "source": [
        "- DataFrame merupakan kumpulan dari banyak Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "804yOd8XU0ee"
      },
      "source": [
        "### Grouping and Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYBcMOf8GDNC"
      },
      "source": [
        "- Grouping adalah teknik **mengelompokkan** data berdasarkan grup dari kolom tertentu, dengan **agregasi** data (seperti min, max, sum, count, dll.) dari kolom lainnya untuk setiap kelompoknya "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2QOxwNNXOV"
      },
      "source": [
        "# Grouping and Aggregation\r\n",
        "class_score = df.groupby('Class').agg({'Score':['mean','min','max']})\r\n",
        "class_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n9ezQkZHEUX"
      },
      "source": [
        "- Contoh diatas adalah pengelompokan kolom **Class** dengan agregasi **mean, min,** dan **max** dari kolom **Score** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3zMhNzoNMMw"
      },
      "source": [
        "# Grouping by Multiple Columns\r\n",
        "class_score = df.groupby(['Class','Age']).agg({'Score':['mean','min','max','count']})\r\n",
        "class_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq9n1Q94Hi-h"
      },
      "source": [
        "- Contoh diatas adalah pengelompokan berdasarkan multi kolom **Class** dan **Age** secara terurut, dengan agregasi **mean, min, max,** dan **count** dari kolom **Score** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS8r0YYhVDLD"
      },
      "source": [
        "### Filtering / Select Rows (biasanya untuk dataset yang lebih besar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWphaPMeMhpe"
      },
      "source": [
        "- Untuk dataset yang lebih besar, [Download Titanic Dataset](https://www.kaggle.com/c/titanic/data), kemudian drag and drop ke colab storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NqhgwRVMYqW"
      },
      "source": [
        "- Pandas membaca data dengan format data seperti yang telah disebutkan sebelumnya (kasus disini adalah format data csv) untuk dimuat kedalam bentuk DataFrame "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN2Fz0B_PMwl"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oK46h9TIBZx"
      },
      "source": [
        "- Setiap pemanggilan DataFrame secara utuh, akan muncul keterangan dimensi data di bawahnya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7LbBuEQBID"
      },
      "source": [
        "# Select by one criteria\r\n",
        "Pclass_3 = df[ (df['Pclass'] == 3)]\r\n",
        "Pclass_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaP17IUPIT0M"
      },
      "source": [
        "- Memilih atau Filter data berdasarkan kolom Pclass = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmirF_lZSyM5"
      },
      "source": [
        "# Filter null values\r\n",
        "df_not_null = df[ (df['Cabin'].notnull())]\r\n",
        "df_not_null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VWX4cDjIajf"
      },
      "source": [
        "- Memilih atau Filter data berdasarkan kolom Cabin yang tidak NaN atau kosong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJiCWyn_TRZl"
      },
      "source": [
        "# Select by list of criteria \r\n",
        "Specific_Pclass= [2,3]\r\n",
        "selection = df[ (df['Pclass'].isin(Specific_Pclass))]\r\n",
        "selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-6Puj9IjwJ"
      },
      "source": [
        "- Memilih atau Filter data berdasarkan kolom Pclass yang memiliki nilai sesuai dengan objek yang ada di dalam suatu list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Oy0slSUCLc"
      },
      "source": [
        "# Negative symbol\r\n",
        "Specific_Pclass= [2,3]\r\n",
        "selection = df[~(df['Pclass'].isin(Specific_Pclass))]\r\n",
        "selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1SS40IPI0gt"
      },
      "source": [
        "- Memilih atau Filter data berdasarkan kolom Pclass dengan simbol \"~\" sebagai negasi dari kriteria yang telah diberikan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_BNofz-UKaH"
      },
      "source": [
        "# Select by more than one criteria \r\n",
        "Specific_Pclass= [2,3]\r\n",
        "selection = df[ (df['Pclass'].isin(Specific_Pclass)) & (df['Sex'] != 'male')]\r\n",
        "selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrI_VqKuJME8"
      },
      "source": [
        "- Dapat menggunakan operator boolean (\"&\" untuk dan, \"|\" untuk atau) untuk menentukan beberapa kriteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3pABizWU4tS"
      },
      "source": [
        "### Joining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob6_xg_3Jl_f"
      },
      "source": [
        "- Joining adalah teknik penggabungan data, baik penggabungan berdasarkan baris/row maupun kolom/column\r\n",
        "- Berikut adalah langkah-langkah menggabungkan dua DataFrame berdasarkan row dengan mengabaikan index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXQlGVwIUl2f"
      },
      "source": [
        "# Creating two different sample dataset\r\n",
        "student1 = [\"Farid\", \"Rizky\", \"Anna\", \"Dani\", \"Juna\", \"Bayu\", \"Gilang\"]\r\n",
        "classroom1 = [\"A\",\"A\",\"B\",\"A\",\"B\",\"B\",\"B\"]\r\n",
        "age1 = [15,15,16,17,15,18,16]\r\n",
        "score1 = [80,82,95,72,67,98,85]\r\n",
        "data1 = {\"Student\":student1, \"Class\":classroom1, \"Age\":age1, \"Score\":score1}\r\n",
        "df1 = pd.DataFrame(data1)\r\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDsO_v6RVZFi"
      },
      "source": [
        "student2 = [\"Rani\", \"Huda\", \"Yayan\", \"Agus\", \"Hasan\"]\r\n",
        "classroom2 = [\"A\",\"B\",\"A\",\"B\",\"A\"]\r\n",
        "age2 = [18,17,17,15,16]\r\n",
        "score2 = [83,91,78,70,97]\r\n",
        "data2 = {\"Student\":student2, \"Class\":classroom2, \"Age\":age2, \"Score\":score2}\r\n",
        "df2 = pd.DataFrame(data2)\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGWBqsmV-y4"
      },
      "source": [
        "# Concatenate By Row and Column\r\n",
        "df_row = pd.concat([df1,df2], ignore_index=True)\r\n",
        "df_row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBuvPoULKI7a"
      },
      "source": [
        "- Berikut adalah langkah-langkah menggabungkan dua DataFrame berdasarkan column sebagai kunci penggabungan, mirip dengan metode join pada SQL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVSEnZ-1WSIh"
      },
      "source": [
        "# Merge on specific keys\r\n",
        "student3 = df_row.Student.tolist()\r\n",
        "id3 = [1,2,3,4,5,6,7,8,9,10,11,12]\r\n",
        "data3 = {\"Student\":student3, \"id\":id3}\r\n",
        "df3 = pd.DataFrame(data3)\r\n",
        "\r\n",
        "df_merge_col = pd.merge(df_row, df3, on=\"Student\")\r\n",
        "df_merge_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP9lBNeVXO_6"
      },
      "source": [
        "### Pivoting\r\n",
        "\r\n",
        "Fungsi pivot_table Python Pandas membantu kita dengan peringkasan dan konversi dataframe dalam bentuk panjang menjadi dataframe dalam bentuk lebar, dalam berbagai skenario yang kompleks. Di Pandas, fungsi tabel pivot menggunakan bingkai data sederhana sebagai input, dan melakukan operasi yang dikelompokkan yang menyediakan ringkasan data multidimensi. [source](https://cmdlinetips.com/2018/12/pivot-table-in-python-pandas/)\r\n",
        "- Elemen standar pivoting:\r\n",
        " - **index** (baris pengelompokan berdasarkan nilai suatu kolom)\r\n",
        " - **columns** (kolom pengelompokan berdasarkan nilai suatu kolom)\r\n",
        " - **values** (ringkasan yang ditampilkan berdasarkan fungsi agregasi (aggfunc) untuk setiap pengelompokan berdasarkan index dan kolom)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYc2PuLZXRkg"
      },
      "source": [
        "# Create pivot table\r\n",
        "df_merge_col.pivot_table(index=\"Class\", columns=\"Age\", values=\"Score\", aggfunc=['mean','sum'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UFCpyQjN3jb"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zerl6LcgN7n2"
      },
      "source": [
        "Data cleaning merupakan langkah awal yang penting dalam proses analisis data yang melibatkan persiapan dan validasi data, biasanya dilakukan sebelum analisis inti Anda. Pembersihan data bukan hanya kasus penghapusan data yang salah atau menghapus record yang terdapat sel yang hilang hilang, namun teknik imputasi banyak digunakan untuk mengatasi masalah tersebut daripada menghapusnya.  Langkah ini dapat dilakukan sebelum ataupun masuk ke dalam proses EDA jika diperlukan.[source](https://careerfoundry.com/en/blog/data-analytics/what-is-data-cleaning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWVnLecHYUbR"
      },
      "source": [
        "### Remove Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msdcU9DhQnSg"
      },
      "source": [
        "- Menghapus baris yang mengandung missing data/ missing value , dengan menetapkan axis = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Z66NXlYaLv"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "\r\n",
        "# Drop row with missing value\r\n",
        "df = df.dropna(axis=0)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ujuoHpUQ-JL"
      },
      "source": [
        "- Menghapus kolom yang mengandung missing data/ missing value , dengan menetapkan axis = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZQR3KmUY3cc"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "\r\n",
        "# Drop column with missing value\r\n",
        "df = df.dropna(axis=1)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu81xZHqRGif"
      },
      "source": [
        "### Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-X13-XhRXkT"
      },
      "source": [
        "Imputasi dapat dianggap sebagai proses melihat deretan data yang hilang dan kemudian \"menyimpulkan\", atau membuat tebakan yang masuk akal, tentang nilai apa yang seharusnya ada pada tempatnya. Machine learning menyediakan metode yang lebih canggih untuk menangani data yang hilang dan tidak mencukupi dibandingkan dengan metode tradisional. [source](https://medium.com/utility-machine-learning/an-introduction-to-imputation-solving-problems-of-missing-and-insufficient-data-1814c90c3545)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUQvSIOKSNyV"
      },
      "source": [
        "- Disini kami akan memberikan contoh sederhana, yaitu dengan mengisikan missing value dengan angka 0 untuk kolom tertentu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHJPAG6BhoQ7"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "\r\n",
        "# Imputation one columns, fill missing value with 0\r\n",
        "df['Age'] = df['Age'].fillna(0)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzEhzhuETfAh"
      },
      "source": [
        "- Imputasi dengan median dari nilai-nilai kolom tertentu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgHdMNGToWP"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "\r\n",
        "# Imputation one columns, fill missing value with median of a column\r\n",
        "df['Age'] = df['Age'].fillna(value=df['Age'].median())\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZv46pGCSfwX"
      },
      "source": [
        "- Selanjutnya juga dapat mengisikan missing value dengan angka 0 untuk semua kolom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8rdSScZRfW"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')\r\n",
        "\r\n",
        "# Imputation all columns, fill missing value with 0\r\n",
        "df = df.fillna(0)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAdnPkyuaHQq"
      },
      "source": [
        "- Metode Imputasi yang lain (termasuk metode machine learning) dapat menggunakan method Imputer pada library **sklearn** [source](https://sklearn.org/modules/generated/sklearn.preprocessing.Imputer.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlXDN0vFVNBJ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pnlEh1lUTxc"
      },
      "source": [
        "**Setelah diberikan contoh sederhana manipulasi data dan pembersihan data, kami memulai analisis sederhana data Titanic dari proses EDA hingga Evaluasi, dengan asumsi data sudah dipersiapkan sebelumnya**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPhrXJSGVQZc"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhK8luONaOmX"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\r\n",
        "Exploratory Data Analysis mengacu pada proses kritis melakukan penyelidikan awal pada data untuk menemukan pola, untuk menemukan anomali, untuk menguji hipotesis (statistik inferentif) dan untuk memeriksa asumsi dengan bantuan statistik deskriptif dan representasi grafis. [source](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\r\n",
        "- Matplotlib \r\n",
        "- Seaborn\r\n",
        "- Pandas\r\n",
        "- Other Plotting Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um2MIXzpg9Dz"
      },
      "source": [
        "# Import library\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBwLiUBwUFGE"
      },
      "source": [
        "### Exploring The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbx_a64AVoIx"
      },
      "source": [
        "# Read dataset - Titanic Dataset\r\n",
        "df = pd.read_csv('train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H7EYhRUqXZ2"
      },
      "source": [
        "df = df[~df.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1KRF4ZYUJYO"
      },
      "source": [
        "# Data Information\r\n",
        "df.info() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHVSlhgNVwtv"
      },
      "source": [
        "- Data berjumlah 891\r\n",
        "- Terdapat missing value dan beberapa data kategorik yang harus diatasi "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlYxMe5CXb1b"
      },
      "source": [
        "# Handle missing value\r\n",
        "\r\n",
        "# Imputation one columns, fill missing value with median of a column\r\n",
        "df['Age'] = df['Age'].fillna(value=df['Age'].median())\r\n",
        "\r\n",
        "# Drop column with missing value\r\n",
        "df = df.dropna(axis=1)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGb-5dqXyFX"
      },
      "source": [
        "- Kami melakukan penanganan missing value sederhana dengan mengisi kolom **Age** dengan 0, dan menghapus kolom **Cabin**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYl-5zK9WMAG"
      },
      "source": [
        "# Number of Unqie Value in Every Columns \r\n",
        "df.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkCyKN41WaLP"
      },
      "source": [
        "- Kolom **Name** dan **PassengerId** tidak diperlukan karena nilainya selalu unik"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-NzRhZWpEf"
      },
      "source": [
        "# Drop Name and PassengerId \r\n",
        "df = df.drop(['Name','PassengerId'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4PJeJ1P9XDB"
      },
      "source": [
        "### Describe The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiRntMEL9nx3"
      },
      "source": [
        "df.describe() # Hanya menjalankan kolom dengan data numerik"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilk0YXPTXHFf"
      },
      "source": [
        "- Kita dapat melakukan analisis statistik deskriptif dengan satu method untuk setiap kolom numerik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oDhEbq5QoVA"
      },
      "source": [
        "### Pie Diagram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiuOAoyvXDqs"
      },
      "source": [
        "# Pandas plotting with matplotlib\r\n",
        "df['Survived'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,figsize=(10,8))\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHyQOeFkXN06"
      },
      "source": [
        "1. `Menampilkan prosentase nilai pada kolom target \"Survived\" menggunakan pie diagram, yang menunjukkan bahwa label 0 lebih banyak dari label 1, namun tidak terlalu imbalanced.`\r\n",
        "2. `Note: jika data imbalanced dapat menyebabkan model cenderung memprediksi kelas mayoritas, hal ini ditangani dengan beberapa metode sampling/augmentasi data, seperti oversampling dan undersampling, serta dapat menggunakan model dengan pembobotan kelas dan dengan metrik AUC.` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsT8IhGZRix2"
      },
      "source": [
        "### Boxplot\r\n",
        "[Deskripsi Boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\r\n",
        "\r\n",
        "![image.png](https://miro.medium.com/max/656/1*2c21SkzJMf3frPXPAR_gZA.png)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwMQbM5XhEe2"
      },
      "source": [
        "# Seaborn with Matplotlib\r\n",
        "fig=plt.gcf()\r\n",
        "fig.set_size_inches(10,7)\r\n",
        "fig=sns.boxplot(x='Survived', y='Age', data=df, order=[0,1], linewidth=2.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vZdHdCM17IY"
      },
      "source": [
        "1. `Menampilkan boxplot nilai \"Age\" untuk masing-masing label target, terlihat bahwa sebaran dalam IQR \"Age\" untuk masing-masing label hampir mirip`\r\n",
        "2. `Terdapat beberapa data yang keluar dari sebaran (outlier)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4neHfh0k0JQL"
      },
      "source": [
        "### Countplot and Barplot (with subplot)\r\n",
        "\r\n",
        "`Countplot digunakan untuk menghitung jumlah sample, sedangkan Barplot salah satunya digunakan untuk menghitung rate/prosentase dari sample.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cla7pZM0GK5"
      },
      "source": [
        "fig, axarr = plt.subplots(1, 2, figsize=(10,5))\r\n",
        "a = sns.countplot(df['Sex'], ax=axarr[0], palette=('#003f7f','#ff007f')).set_title('Passengers count by sex')\r\n",
        "axarr[1].set_title('Survival rate by sex')\r\n",
        "b = sns.barplot(x='Sex', y='Survived', data=df, palette=('#003f7f','#ff007f'), ci=None, ax=axarr[1]).set_ylabel('Survival rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkcQC7-f1AdQ"
      },
      "source": [
        "1. `Terlihat sekitar 65% penumpang adalah laki-laki sedangkan 35% sisanya adalah perempuan.`\r\n",
        "2. `Hal penting yang perlu diperhatikan di sini adalah bahwa tingkat kelangsungan hidup perempuan empat kali lipat tingkat kelangsungan hidup laki-laki dan ini menjadikan kolom Sex salah satu fitur yang paling informatif.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtigNV4C4H21"
      },
      "source": [
        "fig, axarr = plt.subplots(1,2,figsize=(12,6))\r\n",
        "a = sns.countplot(x='Pclass', hue='Survived', data=df, palette=('#C52219', '#23C552'), ax=axarr[0]).set_title('Survivors and deads count by class')\r\n",
        "ax=axarr[1].set_title('Survival rate by class')\r\n",
        "b = sns.barplot(x='Pclass', y='Survived', data=df, palette=\"Greens\", ci=None, ax=axarr[1]).set_ylabel('Survival rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_3I6794uZk"
      },
      "source": [
        "1. `Ada tiga kelas (Pclass) di kapal dan dari plot tersebut terlihat bahwa jumlah penumpang di kelas tiga lebih banyak daripada jumlah penumpang gabungan kelas satu dan dua.`\r\n",
        "2. `Namun, tingkat kelangsungan hidup berdasarkan kelas tidak sama, lebih dari 60% penumpang kelas satu dan sekitar setengah dari penumpang kelas dua berhasil diselamatkan, sedangkan 75% penumpang kelas tiga tidak dapat selamat dari bencana.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ1xwXuwYUO8"
      },
      "source": [
        "Secara sederhana telah diberikan contoh EDA, dan data tersebut akan dilanjutkan ke encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTdcGYtAXgCc"
      },
      "source": [
        "## Methods for Encoding [1](https://towardsdatascience.com/how-to-encode-categorical-data-d44dde313131) & [2](https://towardsdatascience.com/5-categorical-encoding-tricks-you-need-to-know-today-as-a-data-scientist-73cf75595298) Before Modeling\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddXfp5aaaI9"
      },
      "source": [
        "- **Label Encoding** adalah pengubahan label menjadi bentuk numerik untuk mengubahnya menjadi bentuk yang dapat dibaca mesin. Algoritma pembelajaran mesin kemudian dapat memutuskan dengan cara yang lebih baik tentang bagaimana label tersebut harus dioperasikan. \r\n",
        "- **One Hot Encoding** adalah proses yang dikenal sebagai pengkodean variabel kategori menjadi variabel dummy. Metode pengolahan data ini mengubah kolom kategorikal biner (yes / no, male / female,…) menjadi vektor biner 0/1 dimana 0 menunjukkan tidak adanya baris yang termasuk dalam kategori tersebut. Metode ini bisa rumit jika digunakan untuk variabel multidimensi non-biner yang akan menghasilkan penambahan kolom yang tidak berguna. Misalnya, jika kita memiliki kolom yang mewakili x warna, satu pengkodean panas akan menghasilkan x kolom tambahan (warna_hijau, warna_biru,…). \r\n",
        "- dll"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYfl51xPh7-p"
      },
      "source": [
        "dataset = df.copy()\r\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0s4vtxFXgby"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "# Label Encoding\r\n",
        "encoder = LabelEncoder() # method initialization\r\n",
        "\r\n",
        "# Looping for columns except Survived\r\n",
        "for c in dataset.columns[1:]:  \r\n",
        "    if(dataset[c].dtype=='object'): # if column type = object (like \"Tiket\" and \"Sex\")\r\n",
        "        dataset[c] = encoder.fit_transform(dataset[c])\r\n",
        "    else: # else get the self column value without encode\r\n",
        "        dataset[c] = dataset[c]\r\n",
        "\r\n",
        "dataset.head()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQVOCcZ4du7r"
      },
      "source": [
        "- Kami menggunakan Label Encoding untuk mengubah tipe data kategorikal pada kolom Tiket dan Sex menjadi numerikal dengan angka acak 0-(jumlah_data_unik-1) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6GtYDzJbbZp"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "# OneHot Encoding\r\n",
        "encoder = OneHotEncoder() # method initialization\r\n",
        "# Get all binary expanded columns from column Pclass\r\n",
        "train_X_encoded = encoder.fit_transform(dataset[['Pclass']])\r\n",
        "\r\n",
        "# Get all name expanded columns from column Pclass\r\n",
        "column_name = encoder.get_feature_names(['Pclass'])\r\n",
        "\r\n",
        "# Store to DataFrame\r\n",
        "one_hot_encoded_frame =  pd.DataFrame(train_X_encoded.todense(), columns=column_name).astype(int) # Keep it on integer dtype\r\n",
        "\r\n",
        "# Create the columns name on dataset, and drop the original Pclass column\r\n",
        "dataset[column_name] = one_hot_encoded_frame\r\n",
        "dataset = dataset.drop('Pclass', axis=1)\r\n",
        "\r\n",
        "dataset.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7yiOeneDIE"
      },
      "source": [
        "- Lalu kami memecah kolom Pclass menjadi kolom dengan data biner untuk setiap nilai yang ada di kolo Pclass menggunakan OneHot Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBS0ybHVXbyA"
      },
      "source": [
        "# Introduction to Random Forest Algorithm with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAEBZEyX0jz"
      },
      "source": [
        "Algoritma Random Forest adalah algoritma yang paling umum untuk digunakan dalam kompetisi ML seperti kompetisi Kaggle, Jika Anda pernah mencari algoritma ML yang mudah digunakan dan akurat, Anda kadang akan mendapatkan Random Forest di hasil teratas.\n",
        "Untuk memahami algoritma Random forest, Anda harus terbiasa dengan Decision tree terlebih dahulu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecxpu093YGFz"
      },
      "source": [
        "## Apa itu  Decision Tree?\n",
        "*  Decision Tree adalah model prediksi yang menggunakan sekumpulan aturan biner untuk menghitung nilai target.\n",
        "* Ada dua jenis pohon keputusan yaitu pohon klasifikasi dan regresi\n",
        "pohon.\n",
        "*  Decision Tree Classifier digunakan untuk membuat output berupa data kategorikal seperti keputusan ya dan tidak.\n",
        "*  Decision Tree Regression digunakan untuk membuat output berupa data kontinu seperti nilai prosentase.\n",
        "* Setiap Tree adalah model yang cukup sederhana yang memiliki branches, nodes and leaves.\n",
        "* Node berisi atribut yang bergantung pada fungsi tujuan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZnpzjGDYTn0"
      },
      "source": [
        "Sekarang setelah Anda terbiasa dengan Decision Tree, Anda siap untuk memahami Random Forest.\n",
        "\n",
        "## Apa itu Random Forest?\n",
        "Leo Breiman mendefinisikan itu pada [penelitiannya](https://medium.com/r/?url=https%3A%2F%2Fwww.stat.berkeley.edu%2F~breiman%2Frandomforest2001.pdf), “ Random forests adalah kombinasi prediktor pohon sehingga setiap pohon bergantung pada nilai vektor acak yang diambil sampelnya secara independen dan dengan distribusi yang sama untuk semua pohon di hutan ” \n",
        "\n",
        "Definisi lain “Random Forest adalah pengklasifikasi yang terdiri dari kumpulan pengklasifikasi pohon terstruktur {h(x,Θk ), k=1, ...} di mana {Θk} adalah vektor acak yang terdistribusi secara independen dan identik dan setiap pohon membentuk sebuah unit pilih kelas paling populer di input x ”\n",
        "Singkatnya, Random forest membangun beberapa pohon keputusan dan menggabungkannya untuk mendapatkan prediksi yang lebih akurat dan stabil. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v92kOY_pf7UJ"
      },
      "source": [
        "## Keuntungan dari Random Forests \n",
        "* Dapat digunakan untuk masalah klasifikasi dan regresi\n",
        "* Pengurangan overfitting: dengan rata-rata beberapa pohon, risiko overfitting jauh lebih rendah.\n",
        "* Hutan acak membuat prediksi yang salah hanya jika lebih dari setengah pengklasifikasi dasar salah\n",
        "* Sangat mudah untuk mengukur kepentingan relatif setiap fitur pada prediksi. Sklearn menyediakan pustaka untuk ini\n",
        "\n",
        "Karena itu, ini lebih akurat daripada kebanyakan algoritma lainnya.\n",
        "\n",
        "## Kerugian dari Random Forests\n",
        "* Hutan acak telah diamati untuk menyesuaikan secara berlebihan beberapa set data dengan tugas klasifikasi / regresi yang berisik.\n",
        "* Ini lebih kompleks dan mahal secara komputasi daripada algoritma pohon keputusan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srTTMZkAgNlf"
      },
      "source": [
        "## Terminologi penting tentang Decision Trees dan Random Forest [[1]](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)\n",
        "Mari kita lihat terminologi dasar yang digunakan dengan decision trees dan random forests :\n",
        "1. Root Node: Ini mewakili seluruh populasi atau sampel dan ini selanjutnya dibagi menjadi dua atau lebih set homogen.\n",
        "2. Splitting: Ini adalah proses membagi node menjadi dua atau lebih sub-node.\n",
        "3. Decision Node: Ketika sebuah sub-node terpecah menjadi beberapa sub-node, maka itu disebut node keputusan.\n",
        "4. Leaf/ Terminal Node: Node yang tidak terpecah disebut node Leaf atau Terminal.\n",
        "5. Pruning: Saat kita menghapus sub-node dari simpul keputusan, proses ini disebut pemangkasan. Anda dapat mengatakan proses pemisahan yang berlawanan.\n",
        "6. Branch / Sub-Tree: Sebuah sub-bagian dari seluruh pohon disebut cabang atau sub-tree.\n",
        "7. Parent and Child Node: Node yang dibagi menjadi beberapa sub-node disebut node parent dari sub-node dimana sub-node adalah child dari node parent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svIZEMP-XbFB"
      },
      "source": [
        "Setelah kita mengetahui beberapa hal penting tentang random forest, mari kita gunakan algoritma ini di beberapa dataset, Dalam kasus kita, kita akan menggunakan   [Kaggle's titanic survivors dataset](https://www.kaggle.com/c/titanic/data).\n",
        "\n",
        "Dan kemudian kami akan menggunakan neural network serta beberapa model lain untuk membandingkan hasilnya. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4syv3jgZYSm"
      },
      "source": [
        "## Import needed dependencies for modeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syg4fgOvQALJ"
      },
      "source": [
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkXCEM5XPv7"
      },
      "source": [
        "y = dataset['Survived']\n",
        "X = dataset.drop(['Survived'], axis = 1)\n",
        "\n",
        "# Split the dataset to trainand test data\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmbd2WT-eSZf"
      },
      "source": [
        "- Split data dengan porsi data test 25%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea71LIIVZmTt"
      },
      "source": [
        "## Set the parameters for the random forest model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZWbSS6fJdAa"
      },
      "source": [
        "parameters = {'bootstrap': True,\n",
        "              'min_samples_leaf': 3,\n",
        "              'n_estimators': 50, \n",
        "              'min_samples_split': 10,\n",
        "              'max_features': 'sqrt',\n",
        "              'max_depth': 6,\n",
        "              'max_leaf_nodes': None}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rshkfa_hO90w"
      },
      "source": [
        "## Hyperparameters of Sklearn Random forest classifier[[2]](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) :\n",
        "\n",
        "*\t**bootstrap** : boolean, optional (default=True)\n",
        "\n",
        "> Whether bootstrap samples are used when building trees.\n",
        "\n",
        "*\t**min_samples_leaf** : int, float, optional (default=1)\n",
        "\n",
        "> The minimum number of samples required to be at a leaf node:\n",
        "\n",
        "> - If int, then consider min_samples_leaf as the minimum number.\n",
        "\n",
        "> - If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
        "\n",
        "* **n_estimators** : integer, optional (default=10)\n",
        "> The number of trees in the forest.\n",
        "\n",
        "* \t**min_samples_split** :  int, float, optional (default=2)\n",
        "> The minimum number of samples required to split an internal node:\n",
        "\n",
        "> - If int, then consider min_samples_split as the minimum number.\n",
        "> -\tIf float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
        "\n",
        "*\t**max_features** : int, float, string or None, optional (default=”auto”)\n",
        "> The number of features to consider when looking for the best split:\n",
        "\n",
        "> -\tIf int, then consider max_features features at each split.\n",
        "> -If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
        "> -\tIf “auto”, then max_features=sqrt(n_features).\n",
        "> -\tIf “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
        "> -\tIf “log2”, then max_features=log2(n_features).\n",
        "> -\tIf None, then max_features=n_features.\n",
        "\n",
        "\n",
        "*\t**max_depth** :  integer or None, optional (default=None)\n",
        "> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "\n",
        "*\t**max_leaf_nodes** : int or None, optional (default=None)\n",
        "> Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
        "\n",
        "\n",
        "If you want to learn more about the rest of hyperparameters , check out  [sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op9F-SETZyfP"
      },
      "source": [
        "## Define the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrlHxayCZxqf"
      },
      "source": [
        "RF_model = RandomForestClassifier(**parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6LYxmnZ60C"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWiTrBJmaEn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908c9fad-be5d-4f0f-ec52-af2b21d3c161"
      },
      "source": [
        "RF_model.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=6, max_features='sqrt',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=10,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC4sNiZJaf0U"
      },
      "source": [
        "## Test the trained model on test data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpQcS6O1J-Hm"
      },
      "source": [
        "RF_predictions = RF_model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8y9SjiKONS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1a419d-af82-47b1-df95-6aa3a2b12c02"
      },
      "source": [
        "score = accuracy_score(test_y ,RF_predictions)\n",
        "print('Accuracy Random Forest Model:',score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Random Forest Model: 0.8161434977578476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCM3DYk0NO1z"
      },
      "source": [
        "## Using Neural Networks:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmoxQ0K7dYU_"
      },
      "source": [
        "## Define the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjHxqy9o29IR"
      },
      "source": [
        "# Build a neural network :\n",
        "NN_model = Sequential()\n",
        "\n",
        "NN_model.add(Dense(256, input_dim = X.shape[1], activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(256, activation='relu'))\n",
        "NN_model.add(Dense(1, activation='sigmoid'))\n",
        "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFsE2AIPw_h"
      },
      "source": [
        "Define a checkpoint callback :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEw-m-utHEC6"
      },
      "source": [
        "checkpoint_name = 'BestWeights.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_accuracy', verbose = 1, save_best_only = True, mode ='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PapFqg3bdYVD"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GkUvBi-EETO"
      },
      "source": [
        "t0_nn = time()\r\n",
        "NN_model.fit(train_X, train_y, epochs=150, batch_size=64, validation_split = 0.2, callbacks=callbacks_list)\r\n",
        "train_test_time = time() - t0_nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTePkjbGNb97"
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = './BestWeights.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load weights\n",
        "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0I_erHdYVF"
      },
      "source": [
        "## Test the trained model on test data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82WVFZQEH_bW"
      },
      "source": [
        "predictions = NN_model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80CG8FxGIHM_"
      },
      "source": [
        "# round predictions\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "predictions = rounded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAqyEdrZJFcx"
      },
      "source": [
        "score = accuracy_score(test_y ,predictions)\n",
        "print('Test Accuracy:',score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF81luRn4OEU"
      },
      "source": [
        "## Comparision with other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nunEePJ65Tq"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "# models\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \r\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T7hIEcT4Msq"
      },
      "source": [
        "names = [ 'Neural Network',\r\n",
        "          'Logistic Regression',\r\n",
        "          'SVC',\r\n",
        "          'Gradient Boosting Classifier',\r\n",
        "          'Extra Trees Classifier',\r\n",
        "          'Bagging Classifier',\r\n",
        "          'AdaBoost Classifier',\r\n",
        "          'Gaussian NB',\r\n",
        "          'MLP Classifier',\r\n",
        "          'XGB Classifier',\r\n",
        "          'LGBM Classisfier',\r\n",
        "          \"K Nearest Neighbour Classifier\",\r\n",
        "          \"Decison Tree Classifier\",\r\n",
        "          \"Random Forest Classifier\",\r\n",
        "         ]\r\n",
        "classifiers = [\r\n",
        "    NN_model,           \r\n",
        "    LogisticRegression(),\r\n",
        "    SVC(),\r\n",
        "    GradientBoostingClassifier(),\r\n",
        "    ExtraTreesClassifier(),\r\n",
        "    BaggingClassifier(),\r\n",
        "    AdaBoostClassifier(),\r\n",
        "    GaussianNB(),\r\n",
        "    MLPClassifier(),\r\n",
        "    XGBClassifier(),\r\n",
        "    LGBMClassifier(),\r\n",
        "    KNeighborsClassifier(),\r\n",
        "    DecisionTreeClassifier(),\r\n",
        "    RandomForestClassifier(),\r\n",
        "        ]\r\n",
        "\r\n",
        "# Zipped all architecture\r\n",
        "zipped_clf = zip(names,classifiers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad3U3lVr61Vj"
      },
      "source": [
        "import warnings\r\n",
        "# untuk menghapus warning\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "# fungsi accuracy testing \r\n",
        "def acc_summary(model, train_X, train_y, val_X, val_y):\r\n",
        "    t0 = time()\r\n",
        "    model.fit(train_X, train_y)\r\n",
        "    pred_y = model.predict(val_X)\r\n",
        "    train_test_time = time() - t0\r\n",
        "    accuracy = accuracy_score(val_y, pred_y)*100\r\n",
        "    print(\"accuracy : {0:.2f}%\".format(accuracy))\r\n",
        "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\r\n",
        "    print(\"-\"*80)\r\n",
        "    return accuracy, train_test_time\r\n",
        "\r\n",
        "# fungsi komparator\r\n",
        "def classifier_comparator(train_X,train_y,val_X,val_y,classifier=zipped_clf):\r\n",
        "    result = []\r\n",
        "    for n,c in classifier:\r\n",
        "      if n=='Neural Network': # for only neural network\r\n",
        "        predictions = NN_model.predict(test_X)\r\n",
        "        # round predictions\r\n",
        "        rounded = [round(x[0]) for x in predictions]\r\n",
        "        predictions = rounded\r\n",
        "        score = accuracy_score(val_y, predictions)*100\r\n",
        "        print(\"Test result for {}\".format(n))\r\n",
        "        print(\"train and test time: {0:.2f}s\".format(train_test_time))\r\n",
        "        print(\"accuracy : {0:.2f}%\".format(score))\r\n",
        "        print(\"-\"*80)\r\n",
        "        result.append((c,n,score,train_test_time))\r\n",
        "      else:\r\n",
        "        checker_pipeline = Pipeline([\r\n",
        "            ('classifier', c)\r\n",
        "        ])\r\n",
        "        print(\"Validation result for {}\".format(n))\r\n",
        "        #print(c)\r\n",
        "        clf_acc,tt_time = acc_summary(checker_pipeline,train_X, train_y, val_X, val_y)\r\n",
        "        result.append((c,n,clf_acc,tt_time))\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZw1LGOy64XI"
      },
      "source": [
        "result = classifier_comparator(train_X, train_y, test_X, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8x91J1Hn0ed"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c162ngPOpepI"
      },
      "source": [
        "# Plotting models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJtR806ep4nS"
      },
      "source": [
        "# get name and score models\r\n",
        "n = [v[1] for v in result]\r\n",
        "s = [v[2] for v in result]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axBlDuBDpZlE"
      },
      "source": [
        "# Visualisasi model\r\n",
        "plt.figure()\r\n",
        "sns.barplot(x = n, y = s)\r\n",
        "\r\n",
        "plt.title(\"Plotting Test Result\")\r\n",
        "plt.xlabel('Model Name')\r\n",
        "\r\n",
        "plt.xticks(rotation=45, ha='right', size=10)\r\n",
        "plt.yticks(size=10)\r\n",
        "plt.ylabel('Accuracy Score')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUvVOfMQah9x"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPMtqed2ccWW"
      },
      "source": [
        "# get best models [-1] is random forest and next [0] is index to model to get feature importance, and zipped with X column  \r\n",
        "importances = sorted([(a,b) for a,b in zip(result[-1][0].feature_importances_, X.columns)],reverse=True)\r\n",
        "n = [ni for si,ni in importances]\r\n",
        "s = [si for si,ni in importances]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jevRXXNKhJQL"
      },
      "source": [
        "- Mengambil nilai fitur penting\r\n",
        "- Pilih model:\r\n",
        "  - result[-1][0] : random forest\r\n",
        "  - result[-4][0] : lgb\r\n",
        "  - result[-5][0] : xgb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGfciWea-h0"
      },
      "source": [
        "# Plot the feature importances of the forest\r\n",
        "plt.figure()\r\n",
        "plt.title(\"Feature importances\")\r\n",
        "plt.bar(n,s)\r\n",
        "plt.xticks(rotation=45, ha='right', size=10)\r\n",
        "plt.yticks(size=10)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PeYxR4voOFb"
      },
      "source": [
        "### Ensemble Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs1soVsooOFb"
      },
      "source": [
        "# Install and Import Dependencies\r\n",
        "!pip install vecstack\r\n",
        "from vecstack import stacking "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9js15IgoOFc"
      },
      "source": [
        "# get all models, 1-st level.\r\n",
        "models = [v[0] for v in result[1:]] # tanpa neural network\r\n",
        "print('Number of model:',len(models))\r\n",
        "\r\n",
        "t0 = time()\r\n",
        "\r\n",
        "# Compute stacking features\r\n",
        "S_train, S_test = stacking(models, train_X, train_y, test_X, n_folds = 3, metric=accuracy_score,\r\n",
        "                           shuffle = True, random_state = 7, verbose = 1)\r\n",
        "\r\n",
        "# Initialize 2-nd level model\r\n",
        "model = models[-1]\r\n",
        "\r\n",
        "# Fit 2-nd level model\r\n",
        "model = model.fit(S_train, train_y)\r\n",
        "\r\n",
        "# Predict\r\n",
        "y_test_pred = model.predict(S_test)\r\n",
        "\r\n",
        "train_test_time = time() - t0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc5nTkyJoOFc"
      },
      "source": [
        "## Print vecstack result \r\n",
        "accuracy = accuracy_score(test_y, y_test_pred)*100\r\n",
        "print(\"Stacking accuracy : {0:.2f}%\".format(accuracy))\r\n",
        "print(\"train and test time: {0:.2f}s\".format(train_test_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ0epaXIMv7s"
      },
      "source": [
        "## References :\n",
        "* [The Random Forest Algorithm](https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd)\n",
        "*\t[What are some advantages of using a random forest over a decision tree given that a decision tree is simpler?](https://www.quora.com/What-are-some-advantages-of-using-a-random-forest-over-a-decision-tree-given-that-a-decision-tree-is-simpler)\n",
        "*\t[A Complete Tutorial on Tree Based Modeling from Scratch](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)\n",
        "*\t[Sklearn documents of random forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "* [ImageSource](https://cdn5.vectorstock.com/i/1000x1000/88/99/city-and-forest-vector-1078899.jpg)\n",
        "* [Kaggle - All Classification Models](https://www.kaggle.com/backagain/all-classification-models-eda-pandas-profiling)\n",
        "\n"
      ]
    }
  ]
}